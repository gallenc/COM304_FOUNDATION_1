# Personal Learning Record

|      |      |
|:---- |:---- |
| Course and Year | COM304 Foundation Computing 2024 | 
| Student Name | Vishvsinh Khumansinh Bihola |
| Student Number | Q102631751 |
| Github Account | vishvsinh |

## Introduction
                                                                                                                             
                                  

    

                                                                                                                                                                                 
                 Session 1 notes          



Introduction to GitHub

Github is a web site, now owned by Microsoft, which many programmers and organisations use to host their code. In addition to simply hosting the code in Git, Github also provides many tools which can support the ‘Devops' processes of large teams. These include project documentation and project planning tools, package release hosting, web site hosting, issue and pull request management (Github flow) and CI/CD tools using Github automations.

Creating a project plan

The purpose of this Computer Science project is to develop a comprehensive solution to address a specific problem, relevant programming languages, frameworks, and technologies. The project will focus on the application of theoretical and practical knowledge in areas such as algorithms, data structures, and system design. The final deliverable will be a fully functional software product, documentation, and a presentation to showcase the results.


Short history of computing what I personally learn

The history of computing is a fascinating journey that spans centuries of innovation and development. It begins in ancient times, with tools like the abacus used by civilizationssuch as the Sumerians and Chinese to aid in basic arithmetic. By the 17th century, significant progress was made with devices like Blaise Pascal's , one of the first mechanical calculators, which could perform addition and subtraction. This was followed by Gottfried Wilhelm Leibniz's Step Reckoner, which could handle more complex operations like multiplication and division, laying the groundwork for future computing machines. However, it was Charles Babbage, in the 19th century, who is often credited as the "father of the computer." His design for the Analytical Engine was a visionary mechanical computer that included key features such as memory and the ability to execute conditional branching, concepts that are central to modern computing.
The early 20th century saw more rapid advancements, particularly with the advent of electronic computers. In the 1930s, Alan Turing's work on the Turing Machine established the theoretical foundation for computing, introducing the concept of algorithms and computation as a universal process. During World War II, engineers like Konrad Zuse and the British team working on the Colossus developed some of the first fully functional programmable electronic computers, though these early machines were used primarily for military purposes, such as breaking enemy codes. The post-war era ushered in the first generation of general-purpose computers, with the creation of the ENIAC in 1945, which was capable of solving a wide range of mathematical problems. In 1951, the UNIVAC I became the first commercially produced computer, marking the start of the computer industry.
The 1950s and 1960s saw a shift from vacuum tubes to transistors, which made computers more reliable, smaller, and more affordable. This era also witnessed the introduction of mainframes like IBM's System/360, which allowed businesses to perform complex tasks with much greater efficiency. The 1970s brought about a revolution in personal computing, with the introduction of the Altair 8800 in 1975, a computer kit that sparked the creation of the personal computer market. Soon after, companies like Apple emerged with their own personal computers, such as the Apple II, which became a household name.
The 1980s and 1990s marked the rise of the graphical user interface (GUI), with Microsoft’s Windows 95 and the growing popularity of personal computers in homes and businesses around the world. This era also saw the creation of the World Wide Web by Tim Berners-Lee, revolutionising the way people accessed information and communicated online. The late 1990s and early 2000s saw a shift to mobile computing, with the development of smartphones like the iPhone in 2007, which changed the landscape of personal computing once again, making powerful computers portable and accessible in our pockets.
Today, computing is deeply integrated into every aspect of daily life, from cloud computing that allows businesses to store and access data remotely, to artificial intelligence (AI) and machine learning, which are revolutionising fields ranging from healthcare to finance. Quantum computing, still in its infancy, promises to unlock unimaginable processing power, while blockchain technologies are reshaping industries like finance and supply chain management. As we look to the future, the history of computing continues to unfold, marked by an ongoing quest for innovation that challenges the boundaries of what technology can achieve.

getting started with your Raspberry Pi  what I learned  from this 

When I started exploring the Raspberry Pi, I was amazed at how versatile and accessible this tiny computer is for anyone interested in learning about technology and computing. Initially, I learned that the Raspberry Pi is a low-cost, compact device that can be used for a variety of applications, from basic learning to more advanced projects like home automation, robotics, and even media .







 
Session 2 notes 


What I learned  from this session  

Introducing Computer Architecture

Introducing Computer Architecture
Computer architecture is the design and organisation of the components that make up a computer system. It forms the foundation for how a computer processes information, manages resources, and communicates both internally and externally. At its core, computer architecture defines the structure and operational principles of the hardware, which directly influences software design, performance, and efficiency. Whether you’re using a smartphone, a desktop computer, or a supercomputer, the principles of computer architecture play a pivotal role in how these systems function.
One of the central elements of computer architecture is the Central Processing Unit (CPU). Often referred to as the "brain" of the computer, the CPU performs calculations, executes instructions, and coordinates all of the computer’s operations. It contains several subcomponents that are crucial to its function. The Arithmetic Logic Unit (ALU) handles mathematical and logical operations, while the Control Unit (CU) directs the execution of instructions. Registers, which are small, fast storage locations, temporarily hold data that the CPU needs to access quickly during processing.
Equally important is the memory hierarchy in a computer architecture, which consists of various levels of storage, each optimised for different purposes. At the highest level of speed, you have registers, located directly within the CPU. Just beneath them is cache memory, which stores frequently used data and instructions to reduce the time it takes to access information from slower memory. The main memory or RAM (Random Access Memory) is the central working area for programs and data, while secondary storage devices, like hard drives or solid-state drives, provide long-term data storage. The design of this memory hierarchy ensures that the CPU can access the information it needs in the most efficient manner possible.
In a computer system, the bus system acts as the communication highway between different components. It includes several types of buses: the data bus transfers actual data between components, the address bus directs data to specific locations in memory, and the control bus sends signals to coordinate the operations of all the computer’s parts. This interconnected system of buses enables the smooth transfer of information and instructions across the computer.
The computer’s relationship with the outside world is mediated by input/output (I/O) devices. These devices, such as keyboards, mice, displays, and network interfaces, allow the computer to receive input from users and output results. The I/O system is managed by specialised controllers, ensuring that the computer can interact with a wide range of external devices. The motherboard, which houses the CPU, memory, and I/O connectors, serves as the backbone that physically connects all the components and facilitates communication between them.
There are different types of computer architecture, each suited for specific applications. Von Neumann architecture, named after the mathematician John von Neumann, is the most widely used design. In this model, data and instructions are stored in the same memory space, which is both a strength and a limitation. The Harvard architecture, on the other hand, separates memory for data and instructions, allowing for simultaneous access to both, which improves performance in certain specialised systems, such as embedded systems and digital signal processors (DSPs).
An important distinction in computer architecture is between Reduced Instruction Set Computing (RISC) and Complex Instruction Set Computing (CISC). RISC processors use a smaller set of instructions that can be executed in a single clock cycle, leading to faster performance and simplified hardware design. CISC processors, like the x86 architecture, have a larger set of instructions, some of which can perform multiple operations in a single instruction. While RISC processors are often found in mobile devices and embedded systems, CISC processors dominate personal computers and servers.

Binary and Boolean what  I learned  from this concept

In the realm of computing and digital systems, binary and Boolean are two fundamental concepts that underpin virtually all operations. Both are based on logical and mathematical principles, and together, they provide the foundation for how computers process and store information.
Binary  system
The binary system is a base-2 numeral system, meaning it uses only two digits: 0 and 1. These two digits are known as bits, which is short for "binary digits." In a computer, everything is represented in binary formwhether it's numbers, text, or instructions. Computers are inherently binary machines because their hardware components (such as transistors) can easily represent two states: on and off. These states correspond to 1 and 0 in binary.
A single bit can represent two possible states, but when we combine bits together, we can represent more complex information. For instance, with two bits, we can represent four possible states (00, 01, 10, 11), and with three bits, we can represent eight states, and so on. This scaling of possibilities is what allows computers to store and process vast amounts of information using binary numbers.


Binary and Boolean
In the realm of computing and digital systems, binary and Boolean are two fundamental concepts that underpin virtually all operations. Both are based on logical and mathematical principles, and together, they provide the foundation for how computers process and store information.
Binary System
The binary system is a base-2 numeral system, meaning it uses only two digits: 0 and 1. These two digits are known as bits, which is short for "binary digits." In a computer, everything is represented in binary form—whether it's numbers, text, or instructions. Computers are inherently binary machines because their hardware components (such as transistors) can easily represent two states: on and off. These states correspond to 1 and 0 in binary.
A single bit can represent two possible states, but when we combine bits together, we can represent more complex information. For instance, with two bits, we can represent four possible states (00, 01, 10, 11), and with three bits, we can represent eight states, and so on. This scaling of possibilities is what allows computers to store and process vast amounts of information using binary numbers.
Understanding Binary Numbers
In a binary number system, each position in the number represents a power of 2. For example:
	•	The binary number 1011 represents:
	◦	1 × 2³ (8) + 0 × 2² (4) + 1 × 2¹ (2) + 1 × 2⁰ (1)
	◦	Which equals 8 + 0 + 2 + 1 = 11 in decimal.
This shows how a binary number can be converted into a decimal (base-10) number by evaluating each bit as a power of 2.
Boolean Algebra
On the other hand, Boolean algebra is a branch of mathematics that operates on binary variables. It was developed by the mathematician George Boole in the mid-19th century. Boolean algebra is essential for logic circuits and is used to simplify and solve problems related to binary values, as well as for decision-making processes in computers.
In Boolean algebra, there are three fundamental operations:
	1	AND (∧): This operation results in 1 only if both operands are 1; otherwise, it results in 0.
	◦	Example: 1 AND 1 = 1, 1 AND 0 = 0, 0 AND 0 = 0
	2	OR (∨): This operation results in 1 if at least one of the operands is 1; it only results in 0 when both operands are 0.
	◦	Example: 1 OR 0 = 1, 0 OR 0 = 0, 1 OR 1 = 1
	3	NOT (¬): This operation inverts the value of the operand. If the input is 1, the output will be 0, and vice versa.
	◦	Example: NOT 1 = 0, NOT 0 = 1
Boolean Logic in Computers
In computers, Boolean logic forms the foundation of how hardware like processors, memory, and storage devices operate. Digital circuits use logic gates to perform these Boolean operations. These gates are the physical manifestation of Boolean functions in computer hardware.
	•	AND gates are used in scenarios where multiple conditions must be true simultaneously.
	•	OR gates are used when at least one condition needs to be true for the output to be true.
	•	NOT gates are used to negate or flip the value of a binary digit.
When multiple logic gates are combined, they create more complex circuits like adders, multiplexers, flip-flops, and decoders. These circuits, in turn, form the building blocks for processors and memory systems, allowing for the execution of more advanced operations.
Boolean Expressions and Truth Tables
A Boolean expression is a logical statement that can be simplified to a single Boolean value, either true (1) or false (0). These expressions are constructed using logical operations such as AND, OR, and NOT.
To evaluate a Boolean expression, we often use a truth table. A truth table lists all possible combinations of input values (usually represented as 0s and 1s) and shows the corresponding output for a given Boolean expression. 


        




                                                                                        session3



What  I learned about on more Architecture
Computer architecture encompasses the design, structure, and functionality of the components that make up a computer system. It defines how hardware components are organized and how they interact to execute software instructions, store data, and manage system resources. The fundamental role of computer architecture is to ensure the system operates efficiently, effectively meeting the requirements of both hardware and software. With the rapid pace of technological evolution, understanding computer architecture is more critical than ever, as it forms the foundation for developing high-performance systems, whether it's a smartphone, a server, or a supercomputer.
One of the primary elements in computer architecture is the Central Processing Unit (CPU). Often referred to as the "brain" of the computer, the CPU handles most of the computing work. It executes program instructions and performs essential operations such as arithmetic, logical decisions, and data management. The CPU comprises several crucial components that work together to achieve these tasks. The Arithmetic Logic Unit (ALU) is responsible for executing arithmetic and logical operations, while the Control Unit (CU) manages the sequence of operations and interprets the instructions fetched from memory. Additionally, the CPU contains Registers, small and ultra-fast memory locations used to store temporary data during processing.

￼



                                                                                                                                                                                                                                                                                   session4


What I learned from this  session 




Consolidation in computing refers to the process of simplifying or merging systems, resources, or tasks to optimise performance, reduce costs, and enhance efficiency. It can apply to a variety of IT elements, including hardware, data, applications, and infrastructure. Organisations engage in consolidation to streamline their operations, make the most of their resources, and ensure more effective management of their IT systems.
One of the primary forms of consolidation is hardware consolidation, which involves reducing the number of physical machines in an IT environment. This often happens through the use of virtualisation, where multiple virtual machines can run on a single physical machine. Virtualisation allows for greater efficiency by ensuring that hardware resources are used more effectively, reducing the need for physical servers. It also results in savings on power, cooling, and hardware maintenance. In modern data centres, this consolidation can lead to fewer physical machines to manage, simplifying tasks for IT administrators and improving overall operational efficiency.

Software or application consolidation is the process of reducing the number of software applications in use within an organisation by integrating or replacing them with a single, unified solution. Organisations often end up using multiple software tools that perform similar functions, and consolidating these tools into one platform can reduce redundancy and simplify management. With the rise of cloud-based Software-as-a-Service (SaaS) solutions, this type of consolidation has become more popular as businesses look for more efficient ways to handle their software needs, cutting down on the complexity of managing multiple vendors and applications.
IT infrastructure consolidation is a broader process where an organisation combines different parts of its IT infrastructure—servers, storage, networking—into fewer, more efficient systems. This is often done using converged infrastructure or hyper-converged infrastructure platforms, which integrate computing, storage, and networking resources into a single, unified solution. These platforms simplify the management of IT resources and are scalable, allowing businesses to expand their capabilities more easily. Consolidating infrastructure reduces overhead, minimises the need for various IT silos, and improves overall system performance.
In the realm of cloud computing, cloud consolidation involves centralising an organisation’s various cloud services under one provider or platform. Instead of spreading resources across multiple cloud providers, organisations can streamline their operations by consolidating their cloud workloads into one central cloud environment. This reduces complexities in managing cloud resources and often results in cost savings related to data transfer and vendor contracts. Cloud consolidation makes it easier to ensure security, monitor performance, and scale operations as needed, especially in businesses that rely heavily on cloud computing for data storage and processing.





                                                                                                                                                                                                                                                                                  session 5                                    







                                                                                                                                                                                                                                                                            
  
What I learned from this  session 

Computer languages are fundamental to modern computing, acting as the primary means through which humans communicate with computers. These languages allow programmers to write instructions, or code, that the computer interprets and executes to perform specific tasks. Computer languages can be broadly categorised into two main types: low-level languages and high-level languages. Each serves a unique purpose, with varying degrees of abstraction and control over the computer's hardware.
Low-Level Languages
At the most basic level, a computer’s processor understands machine language, which consists of binary code – a series of 1s and 0s. This is the most primitive form of programming, and it directly corresponds to the hardware instructions of the CPU. Machine language is specific to a particular computer architecture, making it difficult for programmers to write or understand 
Also my have been code in the c programming 
￼


Examples of high-level languages include:
	•	Python: Known for its simplicity and readability, Python is widely used for web development, data science, and automation tasks.
	•	Java: A general-purpose programming language used for building cross-platform applications, particularly in web and mobile development.
	•	C/C++: C is a powerful language known for system-level programming, while C++ builds on C with object-oriented features, widely used in game development, operating systems, and high-performance applications.
	•	JavaScript: Primarily used for web development to create interactive web pages. JavaScript runs in web browsers and is essential for front-end development.



                                                                                                                                                                                                                                                                     Session 6 notes 



My personal research 

An Operating System (OS) is a crucial piece of software that serves as the intermediary between the computer hardware and the software applications that run on the system. It manages hardware resources and provides common services for computer programs. Without an operating system, users would need to interact directly with the hardware, which would be a highly complex and inefficient way to use a computer.
Operating systems enable the functioning of a wide variety of hardware devices, from desktop computers and smartphones to servers and embedded systems. They provide a user-friendly interface and handle critical tasks such as managing files, controlling input/output devices, and facilitating communication between processes.

Source Wikipedia)

￼




Memory management is a crucial function of any operating system, as it governs how the computer’s memory resources are allocated, utilised, and freed up. It ensures that different processes (or programs) running on the system are provided with the necessary memory to execute properly, while also ensuring that the system runs efficiently and without errors like conflicts, memory leaks, or crashes. The role of the operating system in memory management is central to system performance, particularly in modern computing environments where multitasking and memory-intensive applications are common.

Kernel processes user processes 

In modern operating systems, the distinction between kernel processes and user processes is a fundamental concept that shapes how the system operates. Understanding the interaction between these two types of processes is key to understanding how a computer manages resources, executes instructions, and ensures smooth and secure operation.

Kernel processes are responsible for tasks such as:
	•	Memory Management: The kernel handles the allocation and deallocation of memory. It ensures that processes are given the necessary resources and protects the memory space of each process to avoid interference or corruption from other processes.
	•	Process Scheduling: The kernel is responsible for determining which process gets to run at any given time. It uses process scheduling algorithms to allocate CPU time to various processes, optimising the system’s performance and ensuring fairness.
    

                                      
                                                                                            session7                                                          


My learning for this session  what I learned 


The origins of the Linux Operating System my learned

The Linux operating system has an interesting and unique origin story that has made it one of the most significant open-source software projects in computing history. Its creation can be traced back to the early 1990s, and it arose as a response to the need for an open, flexible, and affordable operating system. The story of Linux is one of innovation, collaboration, and the power of the open-source movement.


Linux  is a family of open-source Unix-like operating.





File Systems and Security


In the context of modern computing, file systems and security are fundamental to how data is stored, accessed, and protected within an operating system. The relationship between file systems and security is particularly important, as the integrity, confidentiality, and availability of data depend on both how files are managed and how systems guard against unauthorised access and cyber threats.



Boot System Structure

An operating system is a complex program which needs a lot of processes and data structures to be set up before it is ready to run. The process of starting up an operating system is called the boot process.
PC motherboards have a BIOS (Basic Input Output System) which is a firmware program (i.e. on ROM) that will provide basic diagnostic and configuration functions without loading an operating system. The BIOS is responsible for looking at each of the disks in a pre-de fiend order to find a small program called a boot loader. The BIOS also tells the operating system what hardware is installed on the board.




￼



Key Shell Commands




￼


Package management 

Package management is an essential aspect of system administration, particularly in Linux and other Unix-like operating systems. It refers to the process of installing, updating, configuring, and removing software packages from a system. A package is a collection of files and metadata that includes the software program itself, configuration files, libraries, and other resources needed to run the program.
In Unix-like systems, package managers automate the installation and management of software, ensuring that dependencies are resolved and the system remains up-to-date. They help to avoid conflicts between software versions and simplify the process of installing and managing software.





                                                                                                                                                                                                                                                                                                       Session 8 


My personal  experience why learned this and what I researched 


Cyber Security

Cybersecurity refers to the practice of defending systems, networks, and data from digital attacks, unauthorised access, damage, or theft. In today's increasingly connected world, where much of our daily life revolves around the internet, cybersecurity has become critical to ensuring the integrity, confidentiality, and availability of digital information. Cybersecurity measures help protect both individuals and organisations from a wide array of cyber threats, ranging from malicious hackers and cybercriminals to nation-state actors.
Cybersecurity is not just about defending against attacks but also about creating an environment where information systems remain reliable, resilient, and trustworthy. The increasing reliance on digital technologies for everything from online banking to government services has led to a growing focus on securing these systems from potential risks.


Network Security
	•	Network security involves protecting the integrity, confidentiality, and availability of data and resources as they are transmitted over or accessed through networks. This includes both private networks and the internet. Network security measures include firewalls, intrusion detection and prevention systems (IDPS), encryption, and access control mechanisms.





                                                                                                                            
                                                                                      session9                                                                                    



My personal  experience why learned this and what I researched 



Networking is the practice of connecting computers and devices to share resources, communicate, and exchange data across different mediums, whether it’s over physical cables, wireless signals, or through the internet. It plays a vital role in modern computing by enabling communication and data exchange between systems, making the whole infrastructure of digital interactions possible. Networking allows different devices, such as smartphones, desktops, and servers, to work together in real-time, ensuring that data and information can flow seamlessly across various locations and systems.
At the core of networking is the concept of interconnecting devices. These devices can range from simple home devices, like computers and printers, to complex systems like servers and supercomputers. The connection between these devices is essential for resource sharing, communication, and collaborative tasks. Networks allow users to access shared resources, whether it's a file on a remote server, a printer, or a database. Furthermore, networks facilitate internet access, enabling global communication and data exchange across continents. Without networking, modern technology, from social media to cloud computing, wouldn’t be possible.
A network can be defined by its scale and the geographical area it covers. For instance, a Local Area Network (LAN) is a network limited to a small area, like a home, office, or building. LANs typically use Ethernet cables or Wi-Fi to connect devices and enable communication at high speeds. The efficiency and speed of LANs make them ideal for small to medium-scale applications. On a larger scale, there’s the Wide Area Network (WAN), which spans vast geographic distances, such as between cities, countries, or continents. The most recognisable example of a WAN is the internet itself. WANs are formed by linking multiple LANs through telecommunications infrastructure like fibre optics or satellite connections, allowing for global connectivity.
Network Protocol Layers are essential concepts in computer networking. They represent a structured approach to understanding how communication between devices on a network happens. Each layer has a distinct role, and when combined, they allow different systems to communicate with each other in a seamless and organised manner. The layered approach is most famously depicted in the OSI (Open Systems Interconnection) model, but similar ideas are used in the TCP/IP model, which is more commonly implemented in real-world networks, particularly on the internet.
                                                                                                                                                                                                                                                                                                                                                                               Session 10

My personal  experience why learned this and what I researched 



Internet History and cloud Computing



The history of the Internet is a fascinating story of technological evolution, collaboration, and innovation that has transformed the world in profound ways. The Internet, as we know it today, is a global network that connects millions of private, public, academic, and government networks, allowing users to access information, communicate, and share data.

he origins of the Internet date back to the 1960s, during the Cold War, when the U.S. Department of Defence funded research into creating a robust, fault-tolerant communication network. This led to the development of ARPANET (Advanced Research Projects Agency Network), a project that sought to connect computers at different universities and research institutions to share information.
ARPANET utilised the packet-switching method, a technique that divides data into smaller packets, which are sent separately and can take different routes through the network before being reassembled at their destination. This approach ensured that the network remained functional even if some parts of it failed, an important feature for military communications.
                                                                                                                                                                                                                                                                                                                Session 11
What I learned from this session 
Consolidation in computing refers to the process of simplifying or merging systems, resources, or tasks to optimise performance, reduce costs, and enhance efficiency. It can apply to a variety of IT elements, including hardware, data, applications, and infrastructure. Organisations engage in consolidation to streamline their operations, make the most of their resources, and ensure more effective management of their IT systems.
One of the primary forms of consolidation is hardware consolidation, which involves reducing the number of physical machines in an IT environment. This often happens through the use of virtualisation, where multiple virtual machines can run on a single physical machine. Virtualisation allows for greater efficiency by ensuring that hardware resources are used more effectively, reducing the need for physical servers. It also results in savings on power, cooling, and hardware maintenance. In modern data centres, this consolidation can lead to fewer physical machines to manage, simplifying tasks for IT administrators and improving overall operational efficiency.

Software or application consolidation is the process of reducing the number of software applications in use within an organisation by integrating or replacing them with a single, unified solution. Organisations often end up using multiple software tools that perform similar functions, and consolidating these tools into one platform can reduce redundancy and simplify management. With the rise of cloud-based Software-as-a-Service (SaaS) solutions, this type of consolidation has become more popular as businesses look for more efficient ways to handle their software needs, cutting down on the complexity of managing multiple vendors and applications.
IT infrastructure consolidation is a broader process where an organisation combines different parts of its IT infrastructure—servers, storage, networking—into fewer, more efficient systems. This is often done using converged infrastructure or hyper-converged infrastructure platforms, which integrate computing, storage, and networking resources into a single, unified solution. These platforms simplify the management of IT resources and are scalable, allowing businesses to expand their capabilities more easily. Consolidating infrastructure reduces overhead, minimises the need for various IT silos, and improves overall system performance.
In the realm of cloud computing, cloud consolidation involves centralising an organisation’s various cloud services under one provider or platform. Instead of spreading resources across multiple cloud providers, organisations can streamline their operations by consolidating their cloud workloads into one central cloud environment. This reduces complexities in managing cloud resources and often results in cost savings related to data transfer and vendor contracts. Cloud consolidation makes it easier to ensure security, monitor performance, and scale operations as needed, especially in businesses that rely heavily on cloud computing for data storage and processing.
  
These markdown pages contain my notes and reflections on the material presented in class and also my personal research and learning around the module.

## personal notes

| page    | description |
|:--------|:------------|
|[Prior Experience](../personal_learning_record/priorExperience.md) | A summary of my knowledge entering this module|
|[Segment 1 - Computer Architectures](../personal_learning_record/segment1.md) | Experiments and learning about computer architectures |
|[Segment 2 - Languages and Operating Systems](../personal_learning_record/segment2.md) | Experiments and learning about computer languages and operating systems |
|[Segment 3 - Networking and Cloud Computing](../personal_learning_record/segment3.md) |  Experiments and learning about networking |
|[Personal Reflection](../personal_learning_record/personalReflection.md) |A final reflection on what I have learned during this module |



